{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7082 CEM CODE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFL5RCxemB-3"
      },
      "source": [
        "**INSTALLING PYSPARK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9FRtqD42S1C"
      },
      "source": [
        "!python --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPaDiNO9Vymo"
      },
      "source": [
        "#dowload and install java 8\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbggE-57cYdD"
      },
      "source": [
        "#print working directory\n",
        "#!pwd\n",
        "\n",
        "# Downloading Apache spark binary\n",
        "!wget https://apache.mirrors.nublue.co.uk/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRxKvusodvRS"
      },
      "source": [
        "#extracting file\n",
        "!tar -xvzf spark-3.1.2-bin-hadoop3.2.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iZcipz_eBNP"
      },
      "source": [
        "!ls /content/spark-3.1.2-bin-hadoop3.2\n",
        "\n",
        "#installing FindSpark\n",
        "!pip install findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Tvs9vryewLn"
      },
      "source": [
        "import os \n",
        "os.environ[\"SPARK_HOME\"] = '/content/spark-3.1.2-bin-hadoop3.2'\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vvjccf90fGMi"
      },
      "source": [
        "# creating a spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"PySpark 7082cem\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YSNtsdvmkJP"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TwjS8eRmYoJ"
      },
      "source": [
        "**LOADING THE DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgBHrkD8iKOn"
      },
      "source": [
        "Data= spark.read.csv('/content/drive/MyDrive/Colab Notebooks/7082cem/DataCoSupplyChainDataset.csv')\n",
        "Data.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr56wIeyim6C"
      },
      "source": [
        "# We can set header=true as one of the options. This will read the first row as header\n",
        "\n",
        "Data = spark.read.format('csv').options(header='true').load('/content/drive/MyDrive/Colab Notebooks/7082cem/DataCoSupplyChainDataset.csv')\n",
        "Data.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOs9rgk3mve0"
      },
      "source": [
        "Data.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z4C4q-32fyv"
      },
      "source": [
        "#Converting continous variable in the right format by recasting the columns\n",
        "\n",
        "# Import all from `sql.types`\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Write a custom function to convert the data type of DataFrame columns\n",
        "def convertColumn(Data, names, newType):\n",
        "    for name in names: \n",
        "        Data = Data.withColumn(name, Data[name].cast(newType))\n",
        "    return Data \n",
        "# List of continuous features\n",
        "Numeric_Features  = ['Days for shipping (real)', 'Days for shipment (scheduled)','Benefit per order', 'Sales per customer', 'Late_delivery_risk', 'Category Id','Customer Id', 'Customer Zipcode', 'Department Id', 'Latitude', 'Longitude', 'Order Customer Id', 'order date (DateOrders)', \n",
        "                     'Order Id', 'Order Item Cardprod Id', 'Order Item Discount', 'Order Item Discount Rate', 'Order Item Id', 'Order Item Product Price', 'Order Item Profit Ratio', 'Order Item Quantity', 'Sales', 'Order Profit Per Order', 'Order Item Total', 'Product Card Id', 'Product Category Id', 'Product Price', 'Product Status']\n",
        "# Convert the type\n",
        "Data = convertColumn(Data, Numeric_Features, FloatType())\n",
        "# Check the dataset\n",
        "Data.printSchema()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrGSK5swnIQF"
      },
      "source": [
        "# Get the number of rows in the dataframe\n",
        "Data.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYJ4BonAnrUo"
      },
      "source": [
        "#This gives the number of columns in the dataset\n",
        "len(Data.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k2rIEu9kZ_V"
      },
      "source": [
        "**EXPLORING THE DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riNwSBmGcI8h"
      },
      "source": [
        "# Finding the details of a column\n",
        "Data.describe('Type').show()\n",
        "Data.describe('Product Category Id').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuVF83OLoOr4"
      },
      "source": [
        "Data.select('Customer Id', 'Customer Lname').show(5)\n",
        "Data.select('Department Id', 'Department name').distinct().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEYw96IWy3bX"
      },
      "source": [
        "Data.select('Order status', ).distinct().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQcuB1v6ogmr"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "Data= Data.dropDuplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKLbh1Vwc-rn"
      },
      "source": [
        "Data.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO2fp3rZ0KSE"
      },
      "source": [
        "Data.select('Customer Id', 'Customer Fname').distinct().count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38x628jXsfKe"
      },
      "source": [
        "**Data Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDs4hxY0sjs-"
      },
      "source": [
        "# we need to merge the first and last name columns together. \n",
        "#This will enable us identify different people  with similar last or first names.\n",
        "\n",
        " \n",
        "from pyspark.sql import functions as sf\n",
        "\n",
        "Data=Data.withColumn('Name', sf.concat(sf.col('Customer Lname'),sf.lit(' '),sf.col('Customer Fname')))\n",
        "\n",
        "Data.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vlEDAp0us9u"
      },
      "source": [
        "# Check for missing values in every columns\n",
        "from pyspark.sql.functions import isnan, when, count, col\n",
        "Data.select([count(when(col(c).isNull(), c)).alias(c) for c in Data.columns]).show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzbejck2xY2E"
      },
      "source": [
        "#We fill in the customer zipcode column with 3 missing values with 0\n",
        "Data.na.fill(value=0,subset=[\"Customer Zipcode\"]).show(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9guS5vjryiX0"
      },
      "source": [
        "Data.select([count(when(col(d).isNull(), d)).alias(d) for d in Data.columns]).show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVyEVfbahqTN"
      },
      "source": [
        "len(Data.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMuhwqzC05Zx"
      },
      "source": [
        "#Lets create a column called Fraud. All order status suspected to be fraud will take up the value of 1 while others take 0.\n",
        "#This will help us classify transaction easily.\n",
        "\n",
        "from pyspark.sql.functions import when, lit, col\n",
        "Data= Data.withColumn(\"Fraud\", when(col('Order status') == 'SUSPECTED_FRAUD', lit('1')).otherwise(lit('0')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdehI3QoQmh8"
      },
      "source": [
        "Data.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEGwVR67P1Yf"
      },
      "source": [
        "Data.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4vAY19LzCfK"
      },
      "source": [
        "#We drop unnecessary data from he columns\n",
        "\n",
        "columns_to_drop = ['Customer Email','Product Status','Customer Password','Customer Street','Customer Fname','Customer Lname',\n",
        "           'Latitude','Longitude','Product Description','Product Image','Order Zipcode','shipping date (DateOrders)', 'Shipping Mode','Name','Product Category Id','Product Card Id','Order Status','Order Item Id',\n",
        "           'Order Item Cardprod Id','Order Id','order date (DateOrders)','Order Customer Id','Department Id','Customer Zipcode', 'Customer Id', 'Customer Country', 'Customer City', 'Delivery Status']\n",
        "\n",
        "Data = Data.drop(*columns_to_drop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVEQ9iivOfVt"
      },
      "source": [
        "Data.show(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jhWWa8XRpoP"
      },
      "source": [
        "len(Data.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qkqsxcCs6ym"
      },
      "source": [
        "DATA PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqXQ6rZ2d1f5"
      },
      "source": [
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "Encoder_Data = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(Data) for column in list(set(Data.columns)-set(['date'])) ]\n",
        "\n",
        "\n",
        "pipeline = Pipeline(stages=Encoder_Data)\n",
        "Data_r = pipeline.fit(Data).transform(Data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDM9T3zXkHmc"
      },
      "source": [
        "Data_r.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCmDh-_ISzTK"
      },
      "source": [
        "len(Data_r.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRNBJWINS54x"
      },
      "source": [
        "#Drop duplicated columns\n",
        "\n",
        "drops = ['Type','Category Name','Customer Segment','Customer State','Department Name','Market',\n",
        "           'Order Country','Order Region','Product Name','Order State','Fraud','Late_delivery_risk_index', 'Order Profit Per Order_index','Benefit per order_index','Sales per customer_index','Order Item Discount Rate_index','Days for shipment (scheduled)_index|','Sales_index',\n",
        "           'Order City','Order Item Quantity_index','Order Item Product Price_index','Order Item Total_index', 'Days for shipment (scheduled)_index','Order Item Profit Ratio_index','Days for shipping (real)_index','Category Id_index', 'Category Id', 'Product Price_index', 'Order Item Discount_index']\n",
        "\n",
        "New_data = Data_r.drop(*drops)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DXQihSXViee"
      },
      "source": [
        "len(New_data.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FihyEORAV0oQ"
      },
      "source": [
        "New_data.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe1Wt1oqcS3j"
      },
      "source": [
        "Using a Vector Assembler, all features are put in a feature vector column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz9ULz9o65_M"
      },
      "source": [
        "#Features are put inside a feature vector colum\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "\n",
        "Features= ['Days for shipping (real)', 'Days for shipment (scheduled)', 'Benefit per order', 'Sales per customer', 'Late_delivery_risk','Order Item Discount',\n",
        "           'Order Item Discount Rate', 'Order Item Product Price', 'Order Item Profit Ratio', 'Order Item Quantity', 'Sales', 'Order Item Total', 'Order Profit Per Order',\n",
        "           'Product Price', 'Customer State_index', 'Customer Segment_index', 'Product Name_index', 'Order Region_index', 'Order City_index', 'Market_index', 'Order State_index', 'Category Name_index', 'Type_index', 'Order Country_index', 'Department Name_index']\n",
        "Target= ['Fraud_index']\n",
        "\n",
        "Assemble= VectorAssembler(inputCols=Features, outputCol= 'features')\n",
        "AssembleL= VectorAssembler(inputCols=Target, outputCol= 'label')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVM2c4y3E7HG"
      },
      "source": [
        "Assembled= Assemble.transform(New_data)\n",
        "AssembledL= AssembleL.transform(New_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jZnVvo0FHgD"
      },
      "source": [
        "Assembled.show(5, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX0jTyNbu2pw"
      },
      "source": [
        "Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA_ccoW2FT1I"
      },
      "source": [
        "# Initialize standardScaler function\n",
        "\n",
        "ss = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_Ot8XmUFlh0"
      },
      "source": [
        "# Fit the DataFrame to the scaler\n",
        "scaled_data = ss.fit(Assembled).transform(Assembled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73gEaMIdF-jF"
      },
      "source": [
        "scaled_data.select(\"features\", \"features_scaled\").show(5, truncate=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWFjtgNzUkFD"
      },
      "source": [
        "scaled_data = scaled_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ6gtd9IXUpH"
      },
      "source": [
        "**MACHINE LEARNING MODEL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ljh7qcQXoau"
      },
      "source": [
        "First a classification model is built using Logostics Regression to classify if a transaction is a fraud on not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apQ7oAiDG4If"
      },
      "source": [
        "#Splitting data into train and test set in the ratio 70:30%\n",
        "\n",
        "train, test = scaled_data.randomSplit([.7,.3], seed=1230)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrtTaRH8HRql"
      },
      "source": [
        "train.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvvHESe8HxLR"
      },
      "source": [
        "print(train.count())\n",
        "print(test.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TxunS33TJr5"
      },
      "source": [
        "train.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzLJu7GkTV27"
      },
      "source": [
        "scaled_data.show(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUr-BOmvaVYs"
      },
      "source": [
        "label= AssembledL.select('label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZBTyd7Jahko"
      },
      "source": [
        "label.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xadhMh4bIS0p"
      },
      "source": [
        "LOGISTIC REGRESSION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MITYeMeLH8Yb"
      },
      "source": [
        "from pyspark.ml.classification  import LogisticRegression\n",
        "\n",
        "LR = LogisticRegression(featuresCol= 'features_scaled', labelCol= 'Fraud_index', maxIter=10, regParam=0.3, elasticNetParam=0.8) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiqlFCPlJaj_"
      },
      "source": [
        "# Fit the data to the model\n",
        "LRM = LR.fit(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MR-2iNHg3C3"
      },
      "source": [
        "#Fit into test set\n",
        "Predict= LRM.transform(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1sc_5pWhVaO"
      },
      "source": [
        "Predict.select(\"Fraud_index\", 'features_scaled', 'rawPrediction', 'prediction', 'probability').toPandas().head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2jWA0iQo6tj"
      },
      "source": [
        "DECISION TREE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irVHwwlxo542"
      },
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "DTree = DecisionTreeClassifier(labelCol='Fraud_index', featuresCol= 'features_scaled')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVjBC8SHqWNe"
      },
      "source": [
        "DTM = DTree.fit(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBqJ6zj9rBUg"
      },
      "source": [
        "Predict1= DTM.transform(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJcTz8udrJ9O"
      },
      "source": [
        "Predict1.select(\"Fraud_index\", 'features_scaled', 'rawPrediction', 'prediction', 'probability').toPandas().head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTIHhVYPiEse"
      },
      "source": [
        "MODEL EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J30HtHxyjUuv"
      },
      "source": [
        "Accuracy = Predict.filter(Predict.Fraud_index == Predict.prediction).count() / float(Predict.count())\n",
        "print(\"Accuracy : \", Accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgOZ-P52rPbe"
      },
      "source": [
        "Accuracy1 = Predict1.filter(Predict1.Fraud_index == Predict1.prediction).count() / float(Predict1.count())\n",
        "print(\"Accuracy : \", Accuracy1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpmCJWTwnhxv"
      },
      "source": [
        "y_true = Predict.select(['Fraud_index']).collect()\n",
        "y_pred = Predict.select(['prediction']).collect()\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVCfe1DBcM2Y"
      },
      "source": [
        "y_true1 = Predict1.select(['Fraud_index']).collect()\n",
        "y_pred1 = Predict1.select(['prediction']).collect()\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(y_true1, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}